---
title: "Seafloor microbiome community of Golf of Mexico"
output: html_notebook
---

In this Rstudio markdown (version 2.1), a workflow of reanalysis of a publication ("J. Jistka, et.al") using the dada2 pipeline is documented.

### Version of tools used in this R Markdown:

-  **R** = version 3.6.0
- **dada2** = version 1.12.1
- **phyloseq** = version 1.28.0

raw sequence data was downloaded as sra files from NCBI Bio Project. Then the sra files were converted into fastq file by "fastq-dump" version 2.10.0 with command below

```{engine='bash'}

./fastq-dump --split-files $Accession_Number

```

- **--split-files** = This option dumps each read into separate file. For instance, fastq-dump converts one sra file into two fastq files, which each file is corresponded to forward and reverse reads. From this datasets, only one foward fastq files were generated due to the availability of uploaded file.    

25 samples with different sediment depths from 5 different locations in the Golf of Mexico were selected and downloaded. 

### Loading neccesary packages 

```{r}
library("knitr") 
# library("BiocStyle")
# .cran_packages <- c("ggplot2", "gridExtra")0
# .bioc_packages <- c("dada2", "phyloseq", "DECIPHER", "phangorn")
# .inst <- .cran_packages %in% installed.packages()
# if(any(!.inst)) {
#   install.packages(.cran_packages[!.inst])
# }
# .inst <- .bioc_packages %in% installed.packages()
# if(any(!.inst)) {
#   source("http://bioconductor.org/biocLite.R")
#   biocLite(.bioc_packages[!.inst], ask = F)
# }
# # Load packages into session, and print package version
# sapply(c(.cran_packages, .bioc_packages), require, character.only = TRUE)

library(dada2)
```


### -listing all the fastq files downloaded from the NCBI Bio Project cloud server- 

```{r}
path <- "C:\\Users\\MSI\\AppData\\Local\\Packages\\CanonicalGroupLimited.UbuntuonWindows_79rhkp1fndgsc\\LocalState\\rootfs\\home\\matomo\\jacobs\\BCCB_project\\amplicon_analysis_GOM\\extra_data" # change this directory to where fastq files are stored. 
list.files(path)
```



As shown above, there are 30 fastq files that are downloaded from the cloud database. The file name corresponds to the accession number acquired from the NCBI Bio Project database. Metadata of these files have been uploaded to the github repository of this project. Visit https://github.com/matomoniwano/BCCB_project




### -Specifying forward reads from the files within the directory (Fastq-dump only spitted out one fastq file which contains both pair-end reads)-

```{r}

fnFs <- sort(list.files(path, pattern="_1.fastq", full.name = TRUE))

# Extract sample names, assuming filenames have format: SAMPLENAME_1.fastq
sample.names <- sapply(strsplit(basename(fnFs), "_"), `[`, 1)
```

- **fnFs** = name of variable
- **pattern="_1.fastq** = selecting all the fastq files in the directory that contain a string '_1.fastq' in the file name.



### -visualizing the quality of the fastq files before filtering and trimming. 
```{r}
plotQualityProfile(fnFs[1:2])
```

**fnFs[1:2]** = selecting first two files from the targeted fastq files in the directory.

As shown in the plot graph above, the quality of reads declines at the position of 250 and this would be the cutoff point for the truncation. 

### -Assigning a variable for fastq file names after filtering and trimming.- 
```{r}
# Place filtered files in filtered subdirectory
filtFs <- file.path(path, "filtered", paste0(sample.names, "_F_filt.fastq.gz"))
names(filtFs) <- sample.names

```

In this step, file names after filtering process are assigned in a variable called filFs. The name of file is basically the original file name with '_F_filt' before .fastq extention. 

### - filtering and trimming- 




```{r}
out <- filterAndTrim(
  fwd = fnFs,
  filt = filtFs, 
  truncLen=c(250),
  maxN=0,
  maxEE=c(1),
  truncQ=2,
  rm.phix=TRUE,
  compress=TRUE,
  multithread=TRUE)
head(out)
```
#### Parameter used in 'filterAndTrim'

- **fwd** = The path to the input fastq files. In this case, 'fnFs'.
- **filt** = The path to the output filtered files from fwd. In this case 'filtFs'.
- **trunclen** = Legth of reads after truncation of bases. As shown above, the value for trunclen is decided at 250. 
- **maxN** = After truncation, sequences with more than maxN Ns will be discarded. Since DADA2 requires no Ns, we will stick with default value 0.
- **maxEE** = After truncation, reads with higher than maxEE "expected errors" will be discarded. The maxEE parameter sets the maximum number of "expected errors" allowed in a read. In other words, we want to throw the read away if the read has is likely to have more than value 'maxEE' erroneous base calls. The EE is defined to be the mean of errors that would be observed in a very large collection of sequences where error rate in each read position is occured independently. Expected error is the sum of error probabilities. For instance, 
EE = sum(Probability of an error; the base is incorrect if P is the error probability) = sum(10^(-Q/10)). 
If P = 0.5 that means there is a 50% of chance that the base is wrong. Therefore, large EE number implies that the sum of probabilities of error is large as well, so if maxEE is set to low, then only the reads with small sum of error probabilities can pass through the filter (high quality reads). This time maxEE is set to 1 for the sake of high quality reads filteration and efficiency of computation to process such a large datasets. If filtered reads are too few, then please increase the maxEE value (relaxation of filter). 
- **truncQ** = Truncate reads at the first instance of a quality score less than or equal to truncQ. Default is 2 meaning that reads with quality score of 2 (p error = 0.63096) are automatically truncated since there is a 63% chance of the base being wrong. 
- **rm.phix** = If TRUE, discard reads that match against the phiX genome. Phix bacteriophage genome is typically added to illumina sequencing runs for quality monitoring. 
- **compress** = If TRUE, the output fastq files are gzipped
- **multithread** = if TRUE, input files are filtered in parallel via mclapply. It allows it paralell computation which results in faster processing time. 

### -Checking the quality after the filtering.- 
```{r}
plotQualityProfile(filtFs[1:2])
```
The data is looking good now!

### -Dereplication combines all identical sequenceing reads into "unique sequence" witha corresponding "abundance"-
```{r}
derepFs <- derepFastq(filtFs, verbose=TRUE)

# Name the derep-class object by the sample names. 
names(derepFs) <- sample.names
```
- **verbose** = if TRUE, throw standardR messengeson the intermittent and final status of the dereplication. In this case, it is set to TRUE so that the process of dereplication is show in the intermittent. 
### -learn the error rates -

Every amplicon dataset has a different set of error rates. 
Blackline shows the estimated error rates after convergence of the machine-learning algorithm.
Redline shows the error rates expected under the nominal definition of the Q-score.
Black lines are a good fit fot the observed rates and the errror rates drop with increased quality as expected.

```{r}  
errF <- learnErrors(filtFs, multithread=TRUE)
plotErrors(errF, nominalQ=TRUE) 
```

- **multithread** = If TRUE, multithreading is enabled and the number of availble thread is automatically determined. Just like above, this parameter is set to TRUE for faster computation. 
- **nominalQ** =  If TRUE, plot the expected error rates (red line shown in the graph) if quality scores exactly matched their nominal definition: Q = -10 log10(p_err).

The red line is expected line based on the given quality score, the black line indicates the estimated line, and the black dots shows the observed error frequency in each consensus quality score. Ideally, the black dots should follow the track of the black line. 

From the graph above, the black dots follow the trend of black line. This is looking good. 

### -Sample Inference-

This process contains removal of unique sequences that were produced by error. 
```{r}
dadaFs <- dada(filtFs, err=errF, multithread=TRUE, pool="pseudo")
dadaFs[[1]]
```

- **err** = 16xN numeric matrix, or an object coercible by getErrors such as the output of the learnErrors function operated in the previous step. 'errF' is the name of variable that was used to store the result from the previous step.

- **pool** = If pool = TRUE, the algorithm will pool together all samples prior to sample inference. If pool = FALSE, sample inference is performed on each sample individually. If pool = "pseudo", the algorithm will perform pseudo-pooling between individually processed samples. In other words, when sample A has 1000 copies of of sequence Z while sample B only contains one single copy of sequence Z, sequence Z is likely to be filtered out of sample B although it was a true "singleton" among other sequences in the sample B. This is what is expected to happen when the parameter 'pool' is set to FALSE. On the other hand, when the paramter 'pool' is set to TRUE, it is going to require inconvinient over-workload of computation if the large datasets are processed. For this reason, the parameter "pseudo" option lies somewhere between these two pooling options. Pesudo option contains a two step process i which independent processing is performed twice: First on the raw data alone, and then on the raw data again but informed by priors generated from the first round of processing in the second time. Pseudo-pooling provides a more accurate resolution of ASVs. For this reason, pseudo option is used in this datasets. 



### - Construction of ASV files- 

```{r}
seqtab <- makeSequenceTable(samples = dadaFs)
dim(seqtab)
# inspect distribution of sequence lengths
table(nchar(getSequences(seqtab)))
``` 
The number 57757 implies the number of ASV detected from the sample.  

### -Chimera detection- 
```{r}
seqtab.nochim <- removeBimeraDenovo(unqs = seqtab, method = "consensus", multithread = TRUE, verbose = TRUE)
dim(seqtab.nochim)
sum(seqtab.nochim)/sum(seqtab)
```

Chimera is an errous biological sequence derived from two parent sequences during the process of amplification 


- **unqs** = Object that can be coerced in to one with getUniques. In this case seqtab from the previous step.
- **method** = If "pooled": The samples in the sequence table are all pooled together for bimera identification. If "consensus": The samples in a sequence table are independently checked for bimeras, and a consensus decision on each sequence variant is made. If "per-sample": The samples in a sequence table are independently checked for bimeras, and sequence variants are removed (zeroed-out) from samples independently.
- **verbose** = print verbose text output if TRUE. 

### -Tracking the reads from the pipeline-
```{r}
getN <- function(x) sum(getUniques(x))
track <- cbind(out, sapply(dadaFs, getN), rowSums(seqtab.nochim))
# if processing a single sample, remove the sapply calls
colnames(track) <- c("input", "filtered", "denoisedF", "nonchim")
rownames(track) <- sample.names
head(track)
```

### -Assigning Taxonomy- the general fasta release files can be downloaded from https://unite.ut.ee/repository.php
```{r}
taxa <- assignTaxonomy(
  seq = seqtab.nochim, 
  refFasta = "C:\\Users\\MSI\\AppData\\Local\\Packages\\CanonicalGroupLimited.UbuntuonWindows_79rhkp1fndgsc\\LocalState\\rootfs\\home\\matomo\\jacobs\\BCCB_project\\amplicon_analysis_GOM\\silva_nr_v132_train_set.fa.gz", multithread=TRUE)
taxa.print <- taxa # Removing sequence rownames for display only 
rownames(taxa.print) <- NULL
head(taxa.print)
```

### -Exporting ASV table, fasta file, and taxonomy table. -
```{r}
# Giving our seq headers more manageable names
asv_seqs <- colnames(seqtab.nochim)
asv_headers <- vector(dim(seqtab.nochim)[2], mode="character")

for (i in 1:dim(seqtab.nochim)[2]) {
  asv_headers[i] <- paste(">ASV", i, seq="_")
}

# making and writing out a fast of our final ASV seqs:
asv_fasta <- c(rbind(asv_headers, asv_seqs))
# File name can be changed by modifying "ASVs.fa"
write(asv_fasta, "ASVs_GOM.fa") 


# ASV table
asv_tab <- t(seqtab.nochim)
row.names(asv_tab) <- sub(">", "", asv_headers)
# File name can be changed by modifying "ASVs_counts.tsv"
write.table(asv_tab, "ASVs_GOM.tsv", sep="\t", quote=F, col.names=NA)

# tax table (This is commented out due to memory allocation failure on my laptop)
 asv_tax <- taxa
 row.names(asv_tax) <- sub(">", "", asv_headers)
 write.table(asv_tax, "ASVs_GOM_Taxa.tsv", sep="\t", quote=F, col.names=NA)
```

Now there sould be three output files: one FASTA file, one ASV table, and one taxonomy table.

## Analysis in R

### setting the working directory
```{r}
# setting the right directory
setwd("C:\\Users\\MSI\\AppData\\Local\\Packages\\CanonicalGroupLimited.UbuntuonWindows_79rhkp1fndgsc\\LocalState\\rootfs\\home\\matomo\\jacobs\\BCCB_project\\amplicon_analysis_GOM")
list.files()

# install.packages("phyloseq")
# install.packages("vegan")
#install.packages("DESeq2")
# install.packages("ggplot2")
# install.packages("dendextend")
# install.packages("tidyr")
# install.packages("viridis")
# install.packages("reshape")
```

### loading the packages

```{r}
library("phyloseq")
library("vegan")
library("DESeq2")
library("ggplot2")
#library("dendextend")
library("tidyr")
library("viridis")
library("reshape")
library("dplyr")
```

## Checking the package version 

```{r}
packageVersion("phyloseq")
packageVersion("vegan")
packageVersion("DESeq2")
packageVersion("ggplot2") 
#packageVersion("dendextend")
packageVersion("tidyr")
packageVersion("viridis")
packageVersion("reshape")
```

## Reading the exported data from the directory

```{r}
#write(asv_fasta, file = "ASVs_GOM.fa")
#write.table(asv_tab, file = "ASVs_GOM.tsv", sep = "\t", quote = F, col.names = NA)
#write.table(asv_tax, "ASVs_GOM_Taxa.tsv", sep = "\t", quote = F, col.names = NA)

# moving on to 

rm(list=ls())

#Load ASV table
count_tab <- read.table("ASVs_GOM.tsv", header=T, row.names=1, check.names=F, sep="\t")
#Load taxonomy table
tax_tab <- as.matrix(read.table("ASVs_GOM_Taxa.tsv", header=T, row.names=1, check.names=F, sep = "\t"))
#Load sample matadata file
sample_info_tab <- read.table("sample_metadata.txt", header = T, row.names = 1, check.names = F, sep = "\t")

#Check content of the metadata file
sample_info_tab

```

- **sep** = a string used to separate columns. using sep = "\t" gives tab deliminated output
- **quote** = a logical value or a numeric vector. If TRUE, any character or factor coluns will be surrounded by double quotes. If FALSE, nothing is quoted/
- **col.names** = either a logical value indicating whether the column names of x are to be written along with x, or a character vector of column names to be written. 
- **row.names** = a vector of row names. Setting it to 1 means the name of samples/ASV number is listed in the first column of the table

## Assigning contigency tables and sample metadata into phyloseq object
```{r}
#read ASV table into OTU_table class object
otu_1 <- as.matrix(count_tab)
otu <- otu_table(otu_1, taxa_are_rows = T)
#read taxonomy table into tax_table class object
TAX <- tax_table(tax_tab)
#read metadata file into sample_data class object
samples <- sample_data(sample_info_tab)

#Construct phyloseq object from the component objects above. 
Ps <- phyloseq(otu, TAX, samples)
#Check content of the object
Ps
```

I kept getting an error message " Error in validObject(.Object) : invalid class “phyloseq” object: Component taxa/OTU names do not match. Taxa indices are critical to analysis. Try taxa_names()"

This was solved by replacing the parameter in line 327 'taxa_are_rows = F' to taxa_are_rows = T'

- **taxa_are_rows** =  Logical; of length 1 ignored unless object is a matrix, in which case it is required. Basically it disregards the first column

- **phyloseq()** = this function will create a phyloseq object from its component data. OTU_table class, sample_data casll and taxonomy table class. 

Summary of this phyloseq object is displayed in the output


##creating variables for subset samples from phyloseq object

```{r}
#Normalize number of reads in each sample using median sequencing depth
total = median(sample_sums(Ps))
standf = function(x, t=total) round(t *(x/ sum(x)))
Ps <- transform_sample_counts(Ps, standf)


# Keep taxa of interests from the publication. 
ps_class <- subset_taxa(Ps, Class %in% c("Deltaproteobacteria", "Gammaproteobacteria", "Nitrososphaeria", "Alphaproteobacteria", "Phycisphaerae", "Planctomycetacia", "Anaerolineae", "Gemmatimonadetes", "Nitrososphaeria", "Acidimicrobiia", "Subgroup_22", "Acidobacteria"))

ps_class

# subset samples from whole data set. (In this case different sampling sites.)
DSH08 <- subset_samples(ps_class, sample_id=="DSH08")
DSH09 <- subset_samples(ps_class, sample_id=="DSH09")
DSH10 <- subset_samples(ps_class, sample_id=="DSH10")
IXW250 <- subset_samples(ps_class, sample_id=="IXW250")
IXW500 <- subset_samples(ps_class, sample_id=="IXW500")
IXW750 <- subset_samples(ps_class, sample_id=="IXW750")


```


## producing community composition bar plots
```{r}

#DSH08
DSH08 = tax_glom(DSH08, "Class")
DSH08 <- transform_sample_counts(DSH08, function(x) x/sum(x))

DSH08_p <- plot_bar(DSH08, x="depth", fill = "Class")
pd_DSH08 <- DSH08_p$data %>% 
  as_tibble %>%
  mutate(Class = as.character(Class)) %>%
  replace_na(list(Class = "unknown")) 
Class_abun_DSH08 <- pd_DSH08 %>%
  group_by(Class) %>%
  summarize(Abundance = sum(Abundance)) %>%
  arrange(Abundance)
Class_levels_DSH08 <- Class_abun_DSH08$Class
pd0_DSH08 <- pd_DSH08 %>%
  mutate(Class = factor(Class, Class_levels_DSH08))
ggplot(pd0_DSH08, aes(x = depth, y = Abundance, fill = Class )) + geom_bar(stat="identity") + ggtitle("DSH08 1100m") + scale_x_discrete(name = "Depth (cm) ", limits = c(0, 5, 10, 15, 20))
  
#DSH09

DSH09 = tax_glom(DSH09, "Class")
DSH09 <- transform_sample_counts(DSH09, function(x) x/sum(x))

DSH09_p <- plot_bar(DSH09, x="depth", fill = "Class")
pd_DSH09 <- DSH09_p$data %>% 
  as_tibble %>%
  mutate(Class = as.character(Class)) %>%
  replace_na(list(Class = "unknown")) 
pd0_DSH09 <- pd_DSH09 %>%
  mutate(Class = factor(Class, Class_levels_DSH08))
ggplot(pd0_DSH09, aes(x = depth, y = Abundance, fill = Class )) + geom_bar(stat="identity") + ggtitle("DSH09 2290m") + scale_x_discrete(name = "Depth (cm) ", limits = c(0, 5, 10, 15, 20))

#DSH10

DSH10 = tax_glom(DSH10, "Class")
DSH10 <- transform_sample_counts(DSH10, function(x) x/sum(x))

DSH10_p <- plot_bar(DSH10, x="depth", fill = "Class")
pd_DSH10 <- DSH10_p$data %>% 
  as_tibble %>%
  mutate(Class = as.character(Class)) %>%
  replace_na(list(Class = "unknown")) 
pd0_DSH10 <- pd_DSH10 %>%
  mutate(Class = factor(Class, Class_levels_DSH08))
ggplot(pd0_DSH10, aes(x = depth, y = Abundance, fill = Class )) + geom_bar(stat="identity") + ggtitle("DSH10 1500m") + scale_x_discrete(name = "Depth (cm) ", limits = c(0, 5, 10, 15, 20))

#IXW250

IXW250 = tax_glom(IXW250, "Class")
IXW250 <- transform_sample_counts(IXW250, function(x) x/sum(x))

IXW250_p <- plot_bar(IXW250, x="depth", fill = "Class")
pd_IXW250 <- IXW250_p$data %>% 
  as_tibble %>%
  mutate(Class = as.character(Class)) %>%
  replace_na(list(Class = "unknown")) 
pd0_IXW250 <- pd_IXW250 %>%
  mutate(Class = factor(Class, Class_levels_DSH08))
ggplot(pd0_IXW250, aes(x = depth, y = Abundance, fill = Class )) + geom_bar(stat="identity") + ggtitle("IXW250 583m") + scale_x_discrete(name = "Depth (cm) ", limits = c(0, 5, 10, 15, 20))

#IXW500

IXW500 = tax_glom(DSH08, "Class")
IXW500 <- transform_sample_counts(IXW500, function(x) x/sum(x))

IXW500_p <- plot_bar(IXW500, x="depth", fill = "Class")
pd_IXW500 <- IXW500_p$data %>% 
  as_tibble %>%
  mutate(Class = as.character(Class)) %>%
  replace_na(list(Class = "unknown")) 
pd0_IXW500 <- pd_IXW500 %>%
  mutate(Class = factor(Class, Class_levels_DSH08))
ggplot(pd0_IXW500, aes(x = depth, y = Abundance, fill = Class )) + geom_bar(stat="identity") + ggtitle("IXW500 1010m") + scale_x_discrete(name = "Depth (cm) ", limits = c(0, 5, 10, 15, 20))

#IXW750

IXW750 = tax_glom(IXW750, "Class")
IXW750 <- transform_sample_counts(IXW750, function(x) x/sum(x))

IXW750_p <- plot_bar(IXW750, x="depth", fill = "Class")
pd_IXW750 <- IXW750_p$data %>% 
  as_tibble %>%
  mutate(Class = as.character(Class)) %>%
  replace_na(list(Class = "unknown")) 
pd0_IXW750 <- pd_IXW750 %>%
  mutate(Class = factor(Class, Class_levels_DSH08))
ggplot(pd0_IXW750, aes(x = depth, y = Abundance, fill = Class )) + geom_bar(stat="identity") + ggtitle("IXW750 1440m") + scale_x_discrete(name = "Depth (cm) ", limits = c(0, 5, 10, 15, 20))


```

- **tax_glom()** = This function merges species that have the same taxonomy at a certain taxonomic rank. WHen selecting "Class", class taxonomic rank is selected for variable assignment. 
- **transform_sample_counts()** = This function transforms the sample counts of a taxa abundance matrix according to a user provided function. As shown above 'function(x) x/sum(x)' is used to calculate the relative sequence abundance. 
- **ggplot()** = ggplot initializes a ggplot object.
- **ggtitle()** = this function adds title in the plot. 
- **scale_x_discrete()** = This function allows to modify grid and axis title of x-axis. For instance **limits** parameter is used to set axis points for the plot. 

Class is chosen for taxonomy rank for this bar plot. Function ggtitle() is used to display titile of each bar plot. 


### Generation of heatmap

Let's create a basic heatmap using default parameters
```{r}
#Basic heatmap of whole sample set
plot_heatmap(Ps, method = "NMDS", distance = "bray")

#Heatmap for each sampling site

#Merge abundance count together for each sampling site. 
Ps_sample_id <- merge_samples(Ps, "sample_id")
#plot
plot_heatmap(Ps_sample_id, method = "NMDS", distance = "bray") + labs(y = "ASVs", x = "sampling sites")

```

- **method** = The ordination method to use for organizing the heatmap. NMDS is used as a method to plot a heatmap for this data. 
-**distance** = A character string. The ecological distance method to use in the ordination. "bray" = bray curtis dissimilarity

As shown in the heatmap, it is very clustered and difficult to distinct high abundance ASVs. It can be process in a way that only OTUs with >20% abundance are selected in the heatmap. To do this,,

```{r}
#This command gives an error message. 
#Ps_sample_id <- filter_taxa(Ps_sample_id, function(x) sum(x > total * 0.2) > 0, TRUE)
```
However, it kept getting an error stating that 'OTU abundance data must have non-zero dimensions filter_taxa' and the solution is not yet to be found. This will be posted on Github issue. 


Rarefaction curve with iNEXT package

### Installation of iNEXT package from Cran
```{r}
#Install from Cran
#install.packages("iNEXT")

#import package

library(iNEXT)

#package.version("iNEXT")

```

iNEXT function only allws data frame or contigency table as input, so if samples were grouped or names of columns were changed in phyloseq, it is important to transform a phyloseq oject into a data frame. For my case, column names of the original table are accession numbers from NCBI and the names need to be changed to individual sediment depth of sample extraction with name of sampling site. 
```{r}

#Change name of column by individual sediment depths

otu_table_site <- as(otu_table(Ps_sample_id), "matrix")
if(!taxa_are_rows(Ps_sample_id)){otu_table_site <- t(otu_table_site)}
otu_table_site_df <- as.data.frame(otu_table_site)
otu_table_site_df
```

```{r}
# rarefaction curve for 
out <- iNEXT(count_tab, q=c(0, 2), datatype = "abundance")
out$DataInfo

ggiNEXT(out, type=1, se= TRUE, facet.var = "order", color.var = "site") + theme(legend.text = element_text(size = 7)) + ggtitle("Sample size")
ggiNEXT(out, type=2, se = TRUE, facet.var = "order", color.var = "site") + theme(legend.text = element_text(size = 7)) + ggtitle("Sample completeness curve")
ggiNEXT(out, type=3, se = TRUE, facet.var = "order", color.var = "site") + theme(legend.text = element_text(size = 7)) + ggtitle("Coverage based curve")
```

There are three types of curves plotted above.

- **Type 1** = Sample- size based R/E curve. This curve plots diversity estimates with confidence intervals as a function of sample size up to double the reference sample size.
- **Type 2** = sample completeness curve with confidence intervals. This curve plots the sample coverage with respect to sample size for the same range with type 1 plot
- **Type 3** = Coverage based R/E curve, This curve plots the diversity estimates with confidence intervals as a function of sample coverage up to the maximum coverage obrained from the maximum size described in type 1

- **se** = if TRUE, it plots confidence intervals. 
- **theme()** = Changing legend text font size. 
- **ggtitle()** = add title to plot. 


As displayed in out$Datainfo, the lowest number of individuals per sample is 4864 from sample name SRR6172267. To rarefy the diversities of all localities to the lowest number of observed individuals per localitiy which is 4864, endpoint is set to 4846 and the rarefaction curves are plotted below. 
```{r}

out_4864 <- iNEXT(count_tab, q=0, datatype = "abundance", endpoint = 4846) 
ggiNEXT(out_4864, type = 1, facet.var = "order", color.var = "site") + theme(legend.text = element_text(size = 7)) + ggtitle("endpoint = 4846")
ggiNEXT(out, type=2, se = TRUE, facet.var = "order", color.var = "site") + theme(legend.text = element_text(size = 7)) + ggtitle("endpoint = 4846")
ggiNEXT(out, type=3, se = TRUE, facet.var = "order", color.var = "site") + theme(legend.text = element_text(size = 7)) + ggtitle("endpoint = 4846")

```

See rarefaction curve by sampling sites. 
```{r}
out_by_site <- iNEXT(otu_table_site_df, q=0, datatype = "abundance")
out_by_site$DataInfo
ggiNEXT(out_by_site, type=1, se= TRUE, facet.var = "order", color.var = "site") + ggtitle("by sites")
ggiNEXT(out_by_site, type=2, se = TRUE, facet.var = "order", color.var = "site") + ggtitle("by sites")
ggiNEXT(out_by_site, type=3, se = TRUE, facet.var = "order", color.var = "site")  + ggtitle("by sites")
```


## Plotting Chao1 richness estimator and Shannon diversity estimator. 

```{r}
plot_richness(Ps, measures = c("Chao1", "Shannon"))
```


```{r}
plot_richness(Ps, measures = c("Chao1", "Shannon"), x="sample_id", color="depth") + labs(x = "Sampling sites")
```




### - **q** = iNEXT focuses on three measures of Hill numbers of order q: species richness (q = 0), Shannon diversity (q = 1, the exponential of Shannon entropy) and Simpson diversity (q = 2, the inverse of Simpson concentration)

## Beta Diversity 

Beta diversity measures microbial community differences between samples/environments.  

- Since differences in sampling depths between samples can influence distance/dissimilarity metrics, we first need to somehow normalize across our samples - 
DESeq2 package allows us to normalize samples. 

```{r}
#We have to include the colData and design arguments because they are required but it does not really matter. 
deseq_counts <- DESeqDataSetFromMatrix(count_tab, colData = sample_info_tab, design = ~depth)

# if you get an error "Error in estimateSizeFactorsForMatrix(counts(object), locfunc - locfunc,: every gene contains at least one zero, cannot compute log geometric means", that can be due to the sparse orientation of the count table, run this script below

deseq_counts <- estimateSizeFactors(deseq_counts, type = "poscounts")
deseq_counts_vst <- varianceStabilizingTransformation(deseq_counts)

vst_trans_count_tab <- assay(deseq_counts_vst)

euc_dist_2 <- dist(t(vst_trans_count_tab))
```


```{r}
euc_dist <- dist(t(count_tab))
```


```{r}
euc_clust <- hclust(euc_dist, method = "ward.D2")
euc_clust_2 <- hclust(euc_dist_2, method = "ward.D2")


euc_dend <- as.dendrogram(euc_clust, hang=0.1)
euc_dend_2 <- as.dendrogram(euc_clust_2, hang=0.1)

plot(euc_dend, ylab="Euc. Dist")
plot(euc_dend_2, ylab="Euc. Dist, VST")

```



