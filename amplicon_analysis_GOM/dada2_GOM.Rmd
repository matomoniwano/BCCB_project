---
title: "Seafloor microbiome community of Golf of Mexico"
output: html_notebook
---

In this Rstudio markdown (version 2.1), a workflow of reanalysis of a publication ("J. Jistka, et.al") using the dada2 pipeline is documented.

### Version of tools used in this R Markdown:

-  **R** = version 3.6.0
- **dada2** = version 1.12.1
- **phyloseq** = version 1.28.0

raw sequence data was downloaded as sra files from NCBI Bio Project. Then the sra files were converted into fastq file by "fastq-dump" version 2.10.0 with command below

```{engine='bash'}

./fastq-dump --split-files $Accession_Number

```

- **--split-files** = This option dumps each read into separate file. For instance, fastq-dump converts one sra file into two fastq files, which each file is corresponded to forward and reverse reads. From this datasets, only one foward fastq files were generated due to the availability of uploaded file.    

25 samples with different sediment depths from 5 different locations in the Golf of Mexico were selected and downloaded. 

### Loading neccesary packages 

```{r}
library("knitr") 
# library("BiocStyle")
# .cran_packages <- c("ggplot2", "gridExtra")0
# .bioc_packages <- c("dada2", "phyloseq", "DECIPHER", "phangorn")
# .inst <- .cran_packages %in% installed.packages()
# if(any(!.inst)) {
#   install.packages(.cran_packages[!.inst])
# }
# .inst <- .bioc_packages %in% installed.packages()
# if(any(!.inst)) {
#   source("http://bioconductor.org/biocLite.R")
#   biocLite(.bioc_packages[!.inst], ask = F)
# }
# # Load packages into session, and print package version
# sapply(c(.cran_packages, .bioc_packages), require, character.only = TRUE)

library(dada2)
library(phyloseq)
```


### -listing all the fastq files downloaded from the NCBI Bio Project cloud server- 

```{r}
path <- "C:\\Users\\MSI\\AppData\\Local\\Packages\\CanonicalGroupLimited.UbuntuonWindows_79rhkp1fndgsc\\LocalState\\rootfs\\home\\matomo\\jacobs\\BCCB_project\\amplicon_analysis_GOM\\extra_data" # change this directory to where fastq files are stored. 
list.files(path)
```



As shown above, there are 30 fastq files that are downloaded from the cloud database. The file name corresponds to the accession number acquired from the NCBI Bio Project database. Metadata of these files have been uploaded to the github repository of this project. Visit https://github.com/matomoniwano/BCCB_project




### -Specifying forward reads from the files within the directory (Fastq-dump only spitted out one fastq file which contains both pair-end reads)-

```{r}

fnFs <- sort(list.files(path, pattern="_1.fastq", full.name = TRUE))

# Extract sample names, assuming filenames have format: SAMPLENAME_1.fastq
sample.names <- sapply(strsplit(basename(fnFs), "_"), `[`, 1)
```

- **fnFs** = name of variable
- **pattern="_1.fastq** = selecting all the fastq files in the directory that contain a string '_1.fastq' in the file name.



### -visualizing the quality of the fastq files before filtering and trimming. 
```{r}
plotQualityProfile(fnFs[1:2])
```

**fnFs[1:2]** = selecting first two files from the targeted fastq files in the directory.

As shown in the plot graph above, the quality of reads declines at the position of 250 and this would be the cutoff point for the truncation. 

### -Assigning a variable for fastq file names after filtering and trimming.- 
```{r}
# Place filtered files in filtered subdirectory
filtFs <- file.path(path, "filtered", paste0(sample.names, "_F_filt.fastq.gz"))
names(filtFs) <- sample.names

```

In this step, file names after filtering process are assigned in a variable called filFs. The name of file is basically the original file name with '_F_filt' before .fastq extention. 

### - filtering and trimming- 




```{r}
out <- filterAndTrim(
  fwd = fnFs,
  filt = filtFs, 
  truncLen=c(250),
  maxN=0,
  maxEE=c(1),
  truncQ=2,
  rm.phix=TRUE,
  compress=TRUE,
  multithread=TRUE)
head(out)
```
#### Parameter used in 'filterAndTrim'

- **fwd** = The path to the input fastq files. In this case, 'fnFs'.
- **filt** = The path to the output filtered files from fwd. In this case 'filtFs'.
- **trunclen** = Legth of reads after truncation of bases. As shown above, the value for trunclen is decided at 250. 
- **maxN** = After truncation, sequences with more than maxN Ns will be discarded. Since DADA2 requires no Ns, we will stick with default value 0.
- **maxEE** = After truncation, reads with higher than maxEE "expected errors" will be discarded. The maxEE parameter sets the maximum number of "expected errors" allowed in a read. In other words, we want to throw the read away if the read has is likely to have more than value 'maxEE' erroneous base calls. The EE is defined to be the mean of errors that would be observed in a very large collection of sequences where error rate in each read position is occured independently. Expected error is the sum of error probabilities. For instance, 
EE = sum(Probability of an error; the base is incorrect if P is the error probability) = sum(10^(-Q/10)). 
If P = 0.5 that means there is a 50% of chance that the base is wrong. Therefore, large EE number implies that the sum of probabilities of error is large as well, so if maxEE is set to low, then only the reads with small sum of error probabilities can pass through the filter (high quality reads). This time maxEE is set to 1 for the sake of high quality reads filteration and efficiency of computation to process such a large datasets. If filtered reads are too few, then please increase the maxEE value (relaxation of filter). 
- **truncQ** = Truncate reads at the first instance of a quality score less than or equal to truncQ. Default is 2 meaning that reads with quality score of 2 (p error = 0.63096) are automatically truncated since there is a 63% chance of the base being wrong. 
- **rm.phix** = If TRUE, discard reads that match against the phiX genome. Phix bacteriophage genome is typically added to illumina sequencing runs for quality monitoring. 
- **compress** = If TRUE, the output fastq files are gzipped
- **multithread** = if TRUE, input files are filtered in parallel via mclapply. It allows it paralell computation which results in faster processing time. 

### -Checking the quality after the filtering.- 
```{r}
plotQualityProfile(filtFs[1:2])
```
The data is looking good now!

### -Dereplication combines all identical sequenceing reads into "unique sequence" witha corresponding "abundance"-
```{r}
derepFs <- derepFastq(filtFs, verbose=TRUE)

# Name the derep-class object by the sample names. 
names(derepFs) <- sample.names
```
- **verbose** = if TRUE, throw standardR messengeson the intermittent and final status of the dereplication. In this case, it is set to TRUE so that the process of dereplication is show in the intermittent. 
### -learn the error rates -

Every amplicon dataset has a different set of error rates. 
Blackline shows the estimated error rates after convergence of the machine-learning algorithm.
Redline shows the error rates expected under the nominal definition of the Q-score.
Black lines are a good fit fot the observed rates and the errror rates drop with increased quality as expected.

```{r}  
errF <- learnErrors(filtFs, multithread=TRUE)
plotErrors(errF, nominalQ=TRUE) 
```

- **multithread** = If TRUE, multithreading is enabled and the number of availble thread is automatically determined. Just like above, this parameter is set to TRUE for faster computation. 
- **nominalQ** =  If TRUE, plot the expected error rates (red line shown in the graph) if quality scores exactly matched their nominal definition: Q = -10 log10(p_err).

The red line is expected line based on the given quality score, the black line indicates the estimated line, and the black dots shows the observed error frequency in each consensus quality score. Ideally, the black dots should follow the track of the black line. 

From the graph above, the black dots follow the trend of black line. This is looking good. 

### -Sample Inference-

This process contains removal of unique sequences that were produced by error. 
```{r}
dadaFs <- dada(filtFs, err=errF, multithread=TRUE, pool="pseudo")
dadaFs[[1]]
```






- Construction of ASV files- 

```{r}
seqtab <- makeSequenceTable(dadaFs)
dim(seqtab)
# inspect distribution of sequence lengths
table(nchar(getSequences(seqtab)))
``` 
The number 57757 implies the number of ASV detected from the sample.  

-Chimera detection- 
```{r}
seqtab.nochim <- removeBimeraDenovo(seqtab, method="consensus", multithread=TRUE, verbose=TRUE)
dim(seqtab.nochim)
sum(seqtab.nochim)/sum(seqtab)
```

-Tracking the reads from the pipeline-
```{r}
getN <- function(x) sum(getUniques(x))
track <- cbind(out, sapply(dadaFs, getN), rowSums(seqtab.nochim))
# if processing a single sample, remove the sapply calls
colnames(track) <- c("input", "filtered", "denoisedF", "nonchim")
rownames(track) <- sample.names
head(track)
```

-Assigning Taxonomy- the general fasta release files can be downloaded from https://unite.ut.ee/repository.php
```{r}
taxa <- assignTaxonomy(
  seq = seqtab.nochim, 
  refFasta = "C:\\Users\\MSI\\AppData\\Local\\Packages\\CanonicalGroupLimited.UbuntuonWindows_79rhkp1fndgsc\\LocalState\\rootfs\\home\\matomo\\jacobs\\BCCB_project\\amplicon_analysis_GOM\\silva_nr_v132_train_set.fa.gz", multithread=TRUE)
taxa.print <- taxa # Removing sequence rownames for display only 
rownames(taxa.print) <- NULL
head(taxa.print)
```

-Exporting ASV table, fasta file, and taxonomy table. -
```{r}
# Giving our seq headers more manageable names
asv_seqs <- colnames(seqtab.nochim)
asv_headers <- vector(dim(seqtab.nochim)[2], mode="character")

for (i in 1:dim(seqtab.nochim)[2]) {
  asv_headers[i] <- paste(">ASV", i, seq="_")
}

# making and writing out a fast of our final ASV seqs:
asv_fasta <- c(rbind(asv_headers, asv_seqs))
# File name can be changed by modifying "ASVs.fa"
write(asv_fasta, "ASVs_GOM.fa") 


# ASV table
asv_tab <- t(seqtab.nochim)
row.names(asv_tab) <- sub(">", "", asv_headers)
# File name can be changed by modifying "ASVs_counts.tsv"
write.table(asv_tab, "ASVs_GOM.tsv", sep="\t", quote=F, col.names=NA)

# tax table (This is commented out due to memory allocation failure on my laptop)
 asv_tax <- taxa
 row.names(asv_tax) <- sub(">", "", asv_headers)
 write.table(asv_tax, "ASVs_GOM_Taxa.tsv", sep="\t", quote=F, col.names=NA)
```


